---
title: "Advanced Regression and Prediction"
author: "Mauricio Marcos Fajgenbaun"
date: "7/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Life Expectancy in a Country

## Introduction

Even though there have been many studies undertaken in the past regarding different factors affecting life expectancy, none before have taken into account different inmunization effects and Human Development Index (HDI). Also, most of the research has not been done taking into account both different countries and different years (as in this one). Nevertheless, life expectancy is one of the most important indicators a country has, and therefore an indicator that tries to be improved with time through correct policies. This way, we have to ask ourselves what are the most important variables (thus, what are the most important directions public policy whould take) that helps us predict or understand the value of life expectancy in a Country.

This variable importance is straightforward clear. Hence, this gives motivation to study the relationship of this variables with some others related to economic factors, social factors, health factors inmunization factors and mortality factors. At the same time, it is important to know what variables are not so important in explaining the life expectancy in a country. This way, countries will know where not to put there resources too.

In this dataset, there is information on 193 countries that has been collected from the United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. This is also why data has been collected from the year 2000 to 2015.


## Goal

In this project I want to find the best model to predict life expectancy in a Country. The goal is actually double: I want to predict this, but I am also interested in finding the most important variables to do this prediction. This way, there may be some very complex model that will held good results predicting, but will be very difficult to interpret. This is why sometimes I will also take into account the parsimony of the model, and not only its measures.

Also, to account for how good is a model I will be taking into account the following: prediction, prediction interval, Mean Absolut Error (MAE), R squared or adjusted R squared (depending on the case, sometimes one and sometimes both) to asses how much variability will be explained by the model, and how parsimonious is the model. 


###Defining Colours
```{r}
color_1 <- "blue"
color_2 <- "red"
color_3 <- "yellow"
```

### Installing Libraries

```{r}
library("mice")
library("tidyverse")
library("ggplot2")
library("ggcorrplot")
library("caret")
library("corrplot")
library("FOCI")
library("Boruta")
library("GGally")
library("MASS")
library("olsrr")
library("psych")
library("car")
```
### Setting seed

```{r}
set.seed(45052185)
```


### Reading Dataset

```{r}
data <- read.csv("life_expect.csv")
dim(data)
head(data)
```
As we can see, we have a total of 22 variables (columns: 21 independent features and 1 output) and 2938 rows. As my intention is to predict the life expectancy of a country, given the values of some other characteristics of the country, I will drop the variables that account for the name of the country and the year the meassures were taken. This is because, again, it will not add any important information to the analysis (maybe for other analysis, it would be useful). This will be done soon, once we have fixed the missing values.


# Data Preprocessing

```{r}
summary(data)
```

As we can see, we have a lot of missing value in our dataset. We get some variables, such as population with over 652 missing values! That is plenty. Also, there are some variables that have some values equal to zero. As this is a dataset taken from the WHO, we will consider that the zeros are not a mistake, but specific cases of countries where they registered that specific measure.


```{r}
data$Year <- as.factor(data$Year)
sum(apply(data, 1, anyNA))
```

As we can see, there are 1289 rows being affected by Nans. This is a huge problem, as it is almost 30% of ours rows, meaning that it is not a good idea to drop so much information. 

First, I will fill the missing values of every country, with the same information as the past (or next) year from the same country. This way, there will be consistency in eachs countries measures. This is important, as doing any other imputation with, for instance, the median or mean of the column is not a good idea. If I do this, there may be some countries that get a very ilogical value. What is more, as the countries that have many missing values are mostly very small countries (probably, the smallest in the whoele dataset) imputing in another way may cause that they will for sure get a very much larger value than they should.

Let´s fill some missing values with other values from past or forward years, for the same countries:


```{r}
data <- data %>%
  dplyr::group_by(Country) %>%
  fill(Population,GDP,Life.expectancy,Schooling,Income.composition.of.resources,thinness..1.19.years,thinness.5.9.years,Total.expenditure,Hepatitis.B,BMI,Alcohol,Adult.Mortality, .direction = "downup") %>%
  dplyr::ungroup()
```

Let´s how many missing values are still in the dataset.

```{r}
sum(apply(data, 1, anyNA))
```
We have fixed some missing values. Nevertheless, we still have 818 rows with some of them. It is easy to tell that the biggest problems are with the variables Population and GDP. Let´s see what countries have no information whatsoever regarding their population and drop them.

```{r}
bygroup <- aggregate(Population ~ Country, data=data, function(x) {sum(is.na(x))}, na.action = NULL)
```

Now that I grouped per countries, let´s see if there is any country that has, for example, no value at all for Population column (in any year).

```{r}
no_info <- bygroup$Country[bygroup$Population ==16]
no_info
```
There are 40 countries with no value for population whtsoever. When taking a closer look, we can see that most of the missing values in our dataset come from these countries in the list. So I decide to take them out.

```{r}
data <- filter(data, !Country %in% no_info)
summary(data)
dim(data)
sum(apply(data, 1, anyNA))
```
As we can see, now we only have 178 missing values. We have lost some information by deleting so many rows, but we still have 2298 rows. Now we can do an imputation with the library "mice".

```{r}
data <- mice(data,m=1,method="cart")
data <- complete(data)
attach(data)
```

Now, we don´t have any more missing values, we can keep on working with our dataset.

As explained before, let´s first drop the first and second column of the data frame.
```{r}
dato <- data[,3:22]
dim(dato)
head(dato)
```

```{r}
summary(dato)
```
As we can see I have now 19 predictors, from which 18 are continuous and only 1 is categorical. Here is an explanation of them:

1) Status: categorical variable (either developed or developing)
2) Life Expectancy: target variable (continuous)
3) Adult Mortality: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)
4) Infant Deaths: Number of Infant Deaths per 1000 population
5) Alcohol: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)
6) Percentage Expenditure: Expenditure on health as a percentage of Gross Domestic Product per capita(%)
7) Hepatitis B: Hepatitis B (HepB) immunization coverage among 1-year-olds (%)
8) Measles: Measles - number of reported cases per 1000 population
9) BMI: Average Body Mass Index of entire population
10) Under 5 deaths: Number of under-five deaths per 1000 population
11) Polio: Polio (Pol3) immunization coverage among 1-year-olds (%)
12) Total Expenditure: General government expenditure on health as a percentage of total government expenditure (%)
13) Diphteria: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)
14) HIV: Deaths per 1 000 live births HIV/AIDS (0-4 years)
15) GDP:Gross Domestic Product per capita (in USD)
16) Population: Population of the country
17) Thinness 5-9 years: Prevalence of thinness among children for Age 5 to 9(%)
18) Thinness 10-19: Prevalence of thinness among children and adolescents for Age 10 to 19 (% )
19) Income Composition of Resources: Human Development Index in terms of income composition of resources (index ranging from 0 to 1).
20) Schooling: Number of years of Schooling, in years.
.

### Target Variable: Life Expectancy

As said before, my purpose is to predict the life expectancy of a country. Let´s first explore this variable.

```{r}
describe(dato$Life.expectancy)
```

We can first see that from all our data set the minimum value of life expectancy is 36.30, while the maximum is 89. In average, the countries through the years have a life expectancy of 68.69. About the variability, the standrd deviation is 9.8. Its kurtosis is -0.37. This negative value means our variable has a "platykurtic" distribution, which means that it has a flatter peak and thinner tails compared to a normal distribution. This simply means that more data values are located near the mean and less data values are located on the tails. 

Let´s plot the target variable.

```{r}
ggplot(dato, aes(x=Life.expectancy)) + 
 geom_histogram(aes(y=..density..), colour=color_2, fill=color_3)+
 geom_density(alpha=0.3, fill=color_1)+
labs(x = "Life Expectancy", y = "Density")
```
Here we can see that our target variable has a distribution that can be related to a normal, but as said before with more values close to the mean and not so many in the tails (maybe a gamma distribution).


### Features (predictors)

```{r, echo=TRUE, warning=FALSE, message=FALSE, result="hide"}
features <- setdiff(colnames(dato),'data')
numeric_features <- setdiff(colnames(dato[,2:20]),'dato')
```

Let´s first plot an histogram to see how their univariate distribution behaves:

```{r}
for (f in numeric_features) {
  hist(dato[,f], main=f)
}
```

As we have only one categorical variable (Status) let´s see what is its relationship with our target variable.

```{r}
ggplot(dato, aes(x=Status, y=Life.expectancy, fill=Status)) + 
    geom_boxplot()
```

As it can be seen in the plot, developed country have a larger life expectancy median, and its distribution is much more concentrated. Meaning that there is no so much variability between developed countries. It seems that the median is close to 80. On the other hand, developing countries have a less concentrated life expectancy distribution (the box is larger), having a median lower than 70. This way, we can see that there is a huge difference between developed and developing countries. This should not surprise us, as life expectancy is one of the variables to look at when assesing if a country is (or is not) a developed country.


As I will be using CARET to model and predict my target variable, there is no need of standarization or scaling now, as the package can do it when running the algorithm.

Let´s start to asses the relationship between the variables in the dataset.


```{r}
options(repr.plot.width = 20, repr.plot.height = 16)
coord <- ggparcoord(datos, columns=2:19, alphaLines=0.1, missing = 'mean',
                 groupColumn = 'Life.expectancy', scale="uniminmax") + 
xlab("") +
ylab("")
coord
```


Let´s check the autocorrelation between the variables.

```{r}
correlation <- corrplot(cor(dato[,2:20]),type="upper",method="circle",title="Correlation plot",mar=c(0.1,0.1,0.1,0.1))
```

As we can see, there are some variables that have a strong correlation. For exapmle, there is a perfect correlation between "under five year death" and "infant mortality". This is because they measure almost the same, but with a slight different age interval. There is also a strong correlation between "GDP" and the "percentage of expenditure". This is also logical: the richer the country, the bigger the percentage of GDP destined to the health sector.

Let´s plot "infants deaths" vs. "under five deaths":

```{r}
ggplot(dato, aes(x=under.five.deaths, y=infant.deaths)) + 
    geom_point(size=2, shape=23)+ geom_abline(intercept = 0, slope = 1, colour = "blue") 
```

Let´s see what happens when trying to calculate the VIF of a multiple regression when taking into account all the covariates (to check if the two variables will be producing multicolinearity). Here, I will perform a multiple regression but without splitting the data into training and testing set, as I do not want to predict yet but to asses for multicolinearity.

```{r}
multiple.lm <-  lm(Life.expectancy ~., data=dato)
vif(multiple.lm)
```

As we can see, the VIF of "under five year death" and "infant mortality" are extremely high, meaning that they probably meassure almost the same thing. All together, I will drop the "under five year death" as between both of them is the one that has the less correlation with the target variable and this way, we will be avoiding multicolinearity.


```{r}
datos <- dato[,-10]
setdiff(colnames(datos),'data')
```

### Checking linear dependencies

```{r}
X_matr <- data.matrix(datos)
comboInfo <- findLinearCombos(X_matr)
comboInfo
```
There are no columns that can be expressed as linear combination of other columns. This is fundamental to later have no problems in inverting the variance-covariance matrix when performing regression.

## Modeling and Prediction

First, let´s divide the training and testing set with caret.

```{r}
in_train <- createDataPartition((datos$Life.expectancy), p = 0.8, list = FALSE)  # 80% for training
training <- datos[ in_train,]
testing <- datos[-in_train,]
nrow(training)
nrow(testing)
control <- trainControl(method="cv", number=10)  
```

Let´s explore the traning and testing set to be sure there is no huge bias in any of them.

```{r}
ggplot(training, aes(x=Status, y=Life.expectancy, fill=Status)) + 
    geom_boxplot()
ggplot(testing, aes(x=Status, y=Life.expectancy, fill=Status)) + 
    geom_boxplot()
```

As far as we can tell from these boxplots, the training and testing set look well balanced: among developed and developing countries and life expectancy distributions.

Let´s check what are the most correlated variables with "life expectancy":

```{r}
corr_life <- sort(cor(training[,2:19])["Life.expectancy",], decreasing = T)
corr=data.frame(corr_life)
ggplot(corr,aes(x = row.names(corr), y = corr_life)) + 
  geom_bar(stat = "identity", fill = "lightblue") + 
  scale_x_discrete(limits= row.names(corr)) +
  labs(x = "Predictors", y = "Life Expectancy", title = "Correlations") + 
  theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

As we can see, the variable that has the strongest correlation is Schooling, income composition of resources, followed by BMI, and GDP. On the other hand, there are some variables that hold a strong negative correlation with our target, such as Adult mortality (logicaly), HIV (also logicaly) and other variables that are defined by illnesses that may cause death. The only variable that apparently has no correlation with life expectancy is "Population". Nevertheless, correlation does not necesarily imply causality, so we can not yet asses which of these predictors helps us best to predict life expectancy in a country.


## Benchmark 

For this project, I will use a benchmark in order to know how well or bad am I doing when bilding my predictive models. In this case, I will use a multiple regression with the three more correlated variables to the target: "schooling", "income composition of resources" and "adult mortality".

```{r}
benchFit <- lm(Life.expectancy ~ Schooling + Income.composition.of.resources + Adult.Mortality, data=training)
predictions <- predict(benchFit, newdata=testing)
cor((testing$Life.expectancy), predictions)^2
MAE <- mean(abs(predictions - testing$Life.expectancy))
MAE
```
As we can see, for our benchmark we get an R-square equal to 0.7734, meaning that we can explain 77,34% of the variability of the dataset, and our mean absolute error is of 3.2528.


## Feature Selection

Let´s first see what happens when we perform a multiple linear regression, taking into account all the covariates we have, and see what is the importance of each variable.

```{r}
fit.lm <- lm(Life.expectancy ~., data=training)
summary(fit.lm)
lm_imp <- varImp(fit.lm, scale = F)
lm_imp
```
According to this simple approach, the most important variable would be "HIV", followed by "Adult Mortality", "Schooling" and "Income Composition of Resources". This is interesting as these were the variables that showed a strong correlation with the target variable. This way, we are accounting for the largest t-values in the regression. Let´s see what happens when we are interested in lowest p-values instead:

```{r}
summary(fit.lm)
```
As we can see, through this approach we would say that the most important variables would be the "Development Status", "Adult Mortality", "Infant Deaths", Percentage Expenditure", "BMI", "Polio", "Diphteria", "HIV", "Income Composition of resources" and "Schooling". Between these, the lower p-values are of "Income Composition of resources", "Schooling", "HIV", "BMI" and "Adult Mortality". Results are similar to the ones obtained before, although there are some differences. Let´s take into account that our adjusted R squared is 0.837, so we are explaining big part of the variability in the dataset. 

Let´s check the dignostic plots to asses if the assumptions of the model are being violated or not.

```{r}
plot(fit.lm)
```

As we can see in the plots, the residuals seem to be not very well centered around zero, and there is clearly some heterocedasticity (non constant variance). We can say that the lineality assumption is kind of violated here. For the most part, they seem to behave as gaussian (more or less) and there seems to be some possible outliers (although there are no points outside the red line, on the corners of the forth plot, there are some points that are close it).

```{r}
prLM <- predict(fit.lm, newdata=testing)
cor((testing$Life.expectancy), prLM)^2
mean(abs((testing$Life.expectancy)- prLM))
```

As we can see, when taking into account all the covariates we get a better result than our benchmark. Now, we got a better R-squared, at 0.8132 and our mean absolute error has decreased from 3.2528 to 3.14. This is a good start. But from now on, I will also start paying attention to the adjusted R-squared, as I would like to penalize for the amount of variables used.

I will also run this regression through Caret Package, to be able to do a nicer comparison in the future between all predictive models.

```{r}
lm1 <- train(Life.expectancy ~ .,data=training, method='lm',
                preProcess=c("scale","center"), trControl=control) 
```

```{r}
test_results <- data.frame(Life.expectancy = (testing$Life.expectancy))
test_results$lm1 <- predict(lm1, testing)
postResample(pred = test_results$lm1,  obs = test_results$Life.expectancy)
```

With CARET we have an even better model to predict, as it allows us to do some automatic transformations to the data: now by scaling and centering it. Let´s see a plot of the observed vs. the predicted values of the life expectancy in the testing set:

```{r}
qplot(test_results$lm1, test_results$Life.expectancy) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed")  +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```
Here we can see that there is some noise in the predictions. Even though the results seem to be fitting a linear tendency, there are some big residuals, specially in the corner.

Let´s see what happens when we try again a regression with all the covariates, but now using a general linear model, accounting for the skewness in our target variable: we will consider ir a gamma. This will give us a hint on how to deal with our target variable.

```{r}
lm2 <- train(Life.expectancy ~ .,data=training, method='glm',family="Gamma",
                preProcess=c("scale","center"), trControl=control) 
test_results <- data.frame(Life.expectancy = (testing$Life.expectancy))
test_results$lm2 <- predict(lm2, testing)
postResample(pred = test_results$lm2,  obs = test_results$Life.expectancy)
```
When using a gamma family for the glm, we get even a better result than when assuming a gaussian distribution for our target variable. This way, we get an R squared equal to 0.8405 (higher than 0.8384 obtained with the linear model before), a mean absolute error equal to 2.9160 (lower than the 2.9382 from before) and also a lower RMSE. Until now, this is our best model.

### Robust Regression

Now, I will perform two different robust regressions, with all the covariates I have. Robust regression is a special kind of linear regression, where we give different weights to each point, assigned by a specific weighting function. I will use a Huber and a Bisquare to account for possible outliers:

```{r}
linfit.huber <- rlm(Life.expectancy ~ . , data=training)
summary(linfit.huber)

linfit.bisquare <- rlm(Life.expectancy ~ . , data=training, psi = psi.bisquare)
summary(linfit.bisquare)
```
Let´s see how do they predict.

```{r}
prHuber <- predict(linfit.huber, newdata=testing)
cor((testing$Life.expectancy), prHuber)^2
mean(abs((testing$Life.expectancy)- prHuber))

prBi <- predict(linfit.bisquare, newdata=testing)
cor((testing$Life.expectancy), prBi)^2
mean(abs((testing$Life.expectancy)- prBi))

```
Here we get an interesting result. Our R sqaured are lower in comparison to what we got in our previous models, but we predict better in our testing set with both of them. Between them two, apparently the best model to predict is the one with huber function, although they both have very similar results.

### Most important Variables

Let´s try and get a more parsimonious model, when trying also to asses what is maybe the best model given our goal.

First, let´s do a forward based ordinary least of squares regression:

```{r}
forwardpv <- ols_step_forward_p(fit.lm) # forward based on p-value
plot(forwardpv)
```
```{r}
forwardpv$model
```
As we can see, there are 15 variables taken here into account. Let´s see how it predictss:

```{r}
leap1 <- predict(forwardpv$model, newdata=testing)
cor((testing$Life.expectancy), leap1)^2
mean(abs((testing$Life.expectancy)- leap1))
```
Results are not better than the ones we already got, although they are close and with less variables used (has the highest R-squared yet but not the lowest MAE).

Let´s now try using the AKAIKE criteria.

First, let´s do forward based elimination, guiding by the AIC criterion:

```{r}
forwaic <- ols_step_forward_aic(fit.lm) # forward based on AIC
forwaic$model
forwpred <- predict(forwaic$model, newdata=testing)
cor((testing$Life.expectancy), forwpred)^2
mean(abs((testing$Life.expectancy)- forwpred))
```

Now, let´s do the same but with backward elimination, also guiding by the AIC criterion:

```{r}
backaic  <- ols_step_backward_aic(fit.lm) # backward AIC
backaic$model
backpred <- predict(backaic$model, newdata=testing)
cor((testing$Life.expectancy), backpred)^2
mean(abs((testing$Life.expectancy)- backpred))
```

As I get both with the backward and forward process of elimination the same result, there is no point in doing the bothsides step akaike elimination. In both cases, I get to a more parsimonious model in comparison with the one of full covariates, but we do not beat the previous models. Here we get an r-squared of 0.8381 and a Mean Absolute Error of 2.94.

Nevertheless, I am interested in finding what happens when the same is done, but accounting for interactions. Now it is highly likely we will get a huge model with many terms, as I will try every possible two-sided interaction (again forward, backwards and both ways, always guided by AIC criterion).

First forward:

```{r,results="hide"}
forwint <- step(fit.lm, scope = . ~ .^2, direction = 'forward')
```

```{r}
summary(forwint)
```
As we suspected, we have a huge (and impossible to interpret) model, given all possible interactions. Nevertheless, our R-squares is at 0.9021, that is a great sign that shows us that we are accounting for more than 90% of the total variability in the dataset with this model. Let´s see how it predicts.

```{r}
test_results$forwint <- predict(forwint, testing)
postResample(pred = test_results$forwint,  obs = test_results$Life.expectancy)
```

```{r}
qplot(test_results$forwint, test_results$Life.expectancy) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

```{r,results="hide"}
bothint <- step(fit.lm, scope = . ~ .^2, direction = 'both')
```


```{r}
summary(bothint)
test_results$bothint <- predict(bothint, testing)
postResample(pred = test_results$bothint,  obs = test_results$Life.expectancy)
```

This is the best model until now both for predicting and for accounting for the variability of the data.

I will define the model of bothsides with interaction, to use in Caret later for a better comparison.

```{r}
Model1 <- Life.expectancy ~ Status + Adult.Mortality + infant.deaths + 
    Alcohol + percentage.expenditure + Hepatitis.B + Measles + 
    BMI + Polio + Total.expenditure + Diphtheria + HIV.AIDS + 
    GDP + Population + thinness..1.19.years + thinness.5.9.years + 
    Income.composition.of.resources + Schooling + Adult.Mortality:HIV.AIDS + 
    BMI:Diphtheria + thinness..1.19.years:thinness.5.9.years + 
    percentage.expenditure:Income.composition.of.resources + 
    Adult.Mortality:Schooling + BMI:Schooling + Income.composition.of.resources:Schooling + 
    HIV.AIDS:Income.composition.of.resources + Alcohol:thinness..1.19.years + 
    Status:Schooling + Status:Income.composition.of.resources + 
    Total.expenditure:thinness..1.19.years + Status:Adult.Mortality + 
    Measles:HIV.AIDS + Polio:Schooling + Diphtheria:Income.composition.of.resources + Alcohol:HIV.AIDS + Status:GDP + HIV.AIDS:GDP + percentage.expenditure:thinness..1.19.years + Status:BMI + Total.expenditure:HIV.AIDS + HIV.AIDS:Schooling + 
    Adult.Mortality:Hepatitis.B + Measles:Total.expenditure + 
    Status:Alcohol + infant.deaths:BMI + Alcohol:Schooling + 
    Status:thinness..1.19.years + Status:Total.expenditure + 
    Hepatitis.B:Measles + infant.deaths:Total.expenditure + Total.expenditure:Population + Alcohol:Population + Status:infant.deaths + Adult.Mortality:Alcohol + 
    percentage.expenditure:GDP + BMI:GDP + Alcohol:percentage.expenditure + 
    infant.deaths:thinness.5.9.years + Hepatitis.B:Diphtheria + 
    Polio:Income.composition.of.resources + BMI:Polio + Alcohol:Polio + 
    percentage.expenditure:Population + Hepatitis.B:HIV.AIDS + 
    Diphtheria:HIV.AIDS + Alcohol:Total.expenditure + Total.expenditure:Income.composition.of.resources + 
    Alcohol:Income.composition.of.resources + infant.deaths:Income.composition.of.resources + 
    Measles:Income.composition.of.resources + Polio:Diphtheria + 
    infant.deaths:GDP + Status:Population + Hepatitis.B:thinness..1.19.years + 
    Hepatitis.B:thinness.5.9.years + Diphtheria:thinness..1.19.years + 
    infant.deaths:Polio + Diphtheria:GDP + Status:Diphtheria + 
    Measles:Polio
```

Let´s plot Observed vs. Predicted values

```{r}
qplot(test_results$bothint, test_results$Life.expectancy) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

Now, let´s see what are the most important variables, and the most important interactions according to this proceadure.

```{r}
modboth <- train(Model1, data = training, 
                    method = "lm", 
                    preProc=c('scale', 'center'),
                    trControl = control)
importance <- varImp(modboth, scale=FALSE)
plot(importance,top=15)
```

As we can see, BMI seems to be now the most important variable and the most important interactions are "Adult Mortality:HIV", "BMI:Schooling". Until now, this is our best model for predicting life expectancy and accounting for the data variability. Nevertheless, it is a huge and impossible model to interpret.

Let´s try also the same model but now through a general linear model, using "Gamma" family.

```{r}
modboth2 <- train(Model1, data = training, 
                    method = "glm", family="Gamma",
                    preProc=c('scale', 'center'),
                    trControl = control)
test_results$modboth2 <- predict(modboth2, testing)
postResample(pred = test_results$modboth2,  obs = test_results$Life.expectancy)
importance <- varImp(modboth2, scale=FALSE)
plot(importance,top=15)
```

Now, the model has not got better. Although is fairly good, in comparison with the rest, it does not improve the one with the same variables and interaction, but assuming normality.

### Forward Regression

Let´s try with other options. First let´s do forward regression with Caret.

```{r}
for_tune <- train(Life.expectancy ~ ., data = training, 
                  method = "leapForward", 
                  preProc=c('scale', 'center'),
                  tuneGrid = expand.grid(nvmax = 4:19),
                  trControl = control)
for_tune
plot(for_tune)
```

Now, let´s inspect which variables are selected and how well does this model perform:

```{r}
coef(for_tune$finalModel, for_tune$bestTune$nvmax)
test_results$frw <- predict(for_tune, testing)
postResample(pred = test_results$frw,  obs = test_results$Life.expectancy)
```
As it can be seen, this is not a great model, neither for the R-squared nor for the MAE or RMSE. We can see the Observed vs. Predicted values as it follows:

```{r}
qplot(test_results$frw, test_results$Life.expectancy) + 
  labs(title="Forward Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

### Backward Regression

Let´s now check with backward regression

```{r}
back_tune <- train(Life.expectancy ~ ., data = training, 
                   method = "leapBackward", 
                   preProc=c('scale', 'center'),
                   tuneGrid = expand.grid(nvmax = 4:19),
                   trControl = control)
back_tune
plot(back_tune)
```

Let´s see what variables have been selected and how the model performs:

```{r}
coef(back_tune$finalModel, back_tune$bestTune$nvmax)
test_results$bw <- predict(back_tune, testing)
postResample(pred = test_results$bw,  obs = test_results$Life.expectancy)
```

Here, we get pretty much the same result as with the forward regression. We can see it in the observed vs. predicted values plot (the similarities with the one before:

```{r}
qplot(test_results$bw, test_results$Life.expectancy) + 
  labs(title="Backward Regression Observed VS Predicted", x="Predicted", y="Observed")  +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```


### Stepwise regression

Let´s now check stepwise regression:

```{r}
step_tune <- train(Life.expectancy ~ ., data = training, 
                   method = "leapSeq", 
                   preProc=c('scale', 'center'),
                   tuneGrid = expand.grid(nvmax = 4:19),
                   trControl = control)
plot(step_tune)
```

Let´s see which variables are selected and how the model performs:

```{r}
coef(step_tune$finalModel, step_tune$bestTune$nvmax)
test_results$seq <- predict(step_tune, testing)
postResample(pred = test_results$seq,  obs = test_results$Life.expectancy)
```

As it is seen here, we still get the same result (or almost the same one) in comparison with backward and forward regression. We can see it (again) in the plot of observed vs. predicted values in testing set:

```{r}
qplot(test_results$seq, test_results$Life.expectancy) + 
  labs(title="Stepwise Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

## Regularization Method

### Ridge Regression

Let´s try now with our first regularizatino method: Ridge regression. Now, a squared magnitude of the coefficient is added as the penalty term to the loss function. This is good to prevent overfitting. This regression has a parameter that has to be tunned: lambda.  If it is zero, then the equation will be the same as multiple linear regression. Instead, if the parameter is large, it will lead to under-fitting. 

The bad thing about this regression is that is not useful for getting a parsimonious model, as it tends to shrink coefficient to near zero. So we can not do feature selection with it.

Let´s run this regression, and see how it performs in general.

```{r}
ridge_grid <- expand.grid(lambda = seq(0, .1, length = 20))
ridge_tune <- train(Life.expectancy ~ ., data = training,
                    method='ridge',
                    preProc=c('scale','center'),
                    tuneGrid = ridge_grid,
                    trControl=control)
plot(ridge_tune)
ridge_tune$bestTune
test_results$ridge <- predict(ridge_tune, testing)
postResample(pred = test_results$ridge,  obs = test_results$Life.expectancy)
```
As we can see, our lambda parameter is pretty small and this may explain why we get similar performance in comparison to the multiple linear regression.


### Lasso

Lasso (least absolute shrinkage and selection operator) performs also regularization. This time, the absolute value of the magnitude of the coefficient is added as the penalty term to the loss funcion. As some variable coefficient can be zero, we can perform feature selection with it.

Here, we also have a tunning parameter: lambda. Let´s see this model performance:

```{r}
lasso_grid <- expand.grid(fraction = seq(.01, 1, length = 20))
lasso_tune <- train(Life.expectancy ~ . , data = training,
                    method='lasso',
                    preProc=c('scale','center'),
                    tuneGrid = lasso_grid,
                    trControl=control)
plot(lasso_tune)
lasso_tune$bestTune
test_results$lasso <- predict(lasso_tune, testing)
postResample(pred = test_results$lasso,  obs = test_results$Life.expectancy)

```

Again, we get very similar results as with Rdige regression and multiple linear regression. Let´s check the variable importante to get some interpretation of what is happening.

```{r}
importance <- varImp(lasso_tune, scale=FALSE)
plot(importance)
```

### Elastic Net

Elastic net regression that combines, in a linear way, lasso regression and ridge regression (their penalties). 

Let´s see if it performs better, although is highly probable it will provide us with a very similar result of the other two.

```{r}
modelLookup('glmnet')
elastic_grid = expand.grid(alpha = seq(0, .2, 0.01), lambda = seq(0, .1, 0.01))
glmnet_tune <- train(Life.expectancy ~. , data = training,
                     method='glmnet',
                     preProc=c('scale','center'),
                     tuneGrid = elastic_grid,
                     trControl=control)
plot(glmnet_tune)
glmnet_tune$bestTune
test_results$glmnet <- predict(glmnet_tune, testing)
postResample(pred = test_results$glmnet,  obs = test_results$Life.expectancy)
```

We get a slightly lower MAE and the same R-squared value as we got in the previous ones.

We can check the value of each coefficient:

```{r}
coef(glmnet_tune$finalModel, glmnet_tune$bestTune$lambda)
```

Let´s see what variables are the most important according to this method:

```{r}
importance <- varImp(glmnet_tune, scale=FALSE)
plot(importance)
coef(glmnet_tune$finalModel, glmnet_tune$bestTune$lambda)[,1]
```

### Principal Component Regression

Let´s do another kind of regression now: principal component regression. Let´s see how it performs:

```{r}
pcr_tune <- train(Life.expectancy ~ . , data = training,
                  method='pcr',
                  preProc=c('scale','center'),
                  tuneGrid = expand.grid(ncomp = 2:8),
                  trControl=control)
plot(pcr_tune)
test_results$pcr <- predict(pcr_tune, testing)
postResample(pred = test_results$pcr,  obs = test_results$Life.expectancy)

```

Here we get a worse result even than before. Our R-squared has gone down to 0.8290 and our MAE is up to 3.06, so this model is worse both for accounting variability of the dataset and also for predicting.

### Partial Least Square Regression (PLS)

Now we will do something similar as PCR, but we will find a linear regression by projecting the predicted values and the observable variables to a new space. Let´s see what happens when we run it:


```{r}
pls_tune <- train(Life.expectancy ~ . , data = training,
                  method='pls',
                  preProc=c('scale','center'),
                  tuneGrid = expand.grid(ncomp = 2:8),
                  trControl=control)
plot(pls_tune)
test_results$pls <- predict(pls_tune, testing)
postResample(pred = test_results$pls,  obs = test_results$Life.expectancy)

```

As we can see, our results are not that bad now. We get a R-square measure 0.8386 and a MAE of 2.927 (and a RMSE of 3.86).



## Some different approaches

Now, we can try and see with different approaches the most important variables in our dataset. First let´s do Recursive Feature Elimination.


### Recursive Feature Elimination


Here, we can use a different function in order to select a different algorithm. I will use: random forest and bagging of trees.

First with Random Forest:

```{r}
modelRFE <- rfe(Life.expectancy ~ ., data=training, sizes=c(1:19), rfeControl=rfeControl(functions=rfFuncs, method="cv", number=10))
print(modelRFE)
predictors(modelRFE)
ggplot(data = modelRFE, metric = "MAE") + theme_bw()
```

Here the result is interesting. According to this approach, the top 5 variables are "HIV", "Income Composition of resources", "Alcohol" and "BMI". Let´s see what happens when we do the same but with the function calling for bagging.

Second, with treebagFuncs:

```{r}
modelRFE <- rfe(Life.expectancy ~ ., data=training, sizes=c(1:19), rfeControl=rfeControl(functions=treebagFuncs, method="cv", number=10))
print(modelRFE)
predictors(modelRFE)
ggplot(data = modelRFE, metric = "MAE") + theme_bw()
```

Now the result is different. According to this, the most important variable is "Adult Mortality", while "HIV" and "BMI" are also included. It is a similar result, although now the top variable is different. 

### Random Forest for variable selection

We can use random forest algorithm to meassure the importance of each variable in predicting the output. Let´s see:

```{r}
fitRF <- train(Life.expectancy ~ .,data=training, method='rf', preProcess="scale", trControl=control) 
importance <- varImp(fitRF, scale=FALSE)
plot(importance)
```

Now the important variables are the same, but in a different order. First we have "income composition of resources" (or HDI), followed by "HIV", "Adult Mortality" and "Schooling". In particular, is interesting how the variable "Schooling" is so important to predict the life expectation in a country, as its relationship with the amount of years that someone lives is not straightforward. It is probable that the more educated someone is, the more opportunities of having a better life and thus, live more years. 

### FOCI

Let´s see what FOCI has to say about the variable importance.

```{r}
y <- training$Life.expectancy
x <- training[,3:19]
foci(y,x)$selectedVar
```

Now, through FUCI, we get the same important variables, although now Thinness from 1-9 years seems to be important, as well as infant deaths and thinness from 5-9 years old. Now HIV is not the most important one, nor the second one.

### Boruta

Boruta is another algorithm to help us check what are the most important variables:

```{r}
boruta <- Boruta(Life.expectancy ~ ., data=training, doTrace=0)  
plot(boruta, cex.axis=.7, las=2, xlab="", main="Variable Importance")
```

According to Boruta, the most important variable is HIV, followed by Adult Mortality, HDI and Alcohol.


Comparing Models

```{r}
mods <- resamples(list(lm =lm1, glm = lm2,glm2 = modboth2, AICstep =modboth, Elasticnet = glmnet_tune,Lasso = lasso_tune, Ridge=ridge_tune,PCR=pcr_tune,PLS=pls_tune))
summary(mods)
```

When compring the different achieved models, we can say that:

a) Regarding R-squared: in average, the best model is glm2: the general linear model that takes a lot of interactions and variables, and uses a gamma family for the target variable. This model can explain 88% of the total variability in the dataset. When chasing a simpler model, the general linear model with gamma family is pretty good too.

b) Regarding MAE: in average the best model is also glm2: as it gets in average a mean absolute error in prediction of 3.292351. When checking the minimums, the model that gets the lowest MAE is the multiple regression with a lot of interactions and variables. When looking for a more parsimonious model, we can find that general linear model with all the variables but no interaction gets to (minimum) 2.71.

## Combining Models

A good way of finding a better model from what has been done until now, is to combine some of them and check if by calculating the average of their predictions we get to a better result. Let´s try!


```{r}
apply(test_results[-1], 2, function(x) cor(x,test_results$Life.expectancy)^2)
apply(test_results[-1], 2, function(x) mean(abs(x - test_results$Life.expectancy)))
```
```{r}
test_results %>%
  dplyr::select(-Life.expectancy) %>%
  ggcorr(palette = "RdBu", label = TRUE) + labs(title = "Correlations between different models")
```
```{r}
test_results$comb = (test_results$modboth + test_results$modboth2   +  test_results$lm2 +test_results$forwint)/4
postResample(pred = test_results$comb,  obs = test_results$Life.expectancy)
```

### Prediction Intervals 

Now it´s time to calculate some intervals for the predictions (different than the regular confidence interval).

I will calculate these prediction intervals with my best linear model first, the one with many variables and interactions.

```{r}
modelo <- lm(Model1,training)
predictions <- predict.lm(modelo, newdata=testing, interval="prediction", level=0.90)
head(predictions)
predictions=as.data.frame(predictions)
predictions$real = (test_results$Life.expectancy)
```

```{r}
predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))
mean(predictions$out==1)
```
We can see that somewhere around 10% of the real observations lay outside of the prediction interval. Let´s check it in the plot:

```{r}
ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  xlim(10, 100) + ylim(10, 100)+
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "prediction",y="real life expectancy")
```
As we can see, our prediction intervals are pretty good, as less than 10% (slightly) of the rel values are outside of the interval, and most of them are inside.



# But what to do if we want to use a model with better prediction performance?
# For instance, use the combination model
# Then we cannot use the predict() function...

# Final predictions:
yhat = exp(test_results$comb)
head(yhat) # show the prediction for 6 home prices
hist(yhat, col="lightblue")
# take care: asymmetric distribution

y = exp(test_results$price)
error = y-yhat
hist(error, col="lightblue")
# But the error is more symmetric

# But we cannot use the information in the testing set to obtain the confidence intervals in the testing set
# Hence: split the testing set in two parts, one to measure the size of the noise, and the other one to compute the CIs from that size

# Let's use the first 100 homes in testing to compute the noise size
noise = error[1:100]
sd.noise = sd(noise)

# Let's use the rest of the homes to validate the coverage
# If noise is a normal variable, then a 90%-CI can be computed as
lwr = yhat[101:length(yhat)] - 1.65*sd.noise
upr = yhat[101:length(yhat)] + 1.65*sd.noise

# Non-parametric way:
lwr = yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr = yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)

# Non-parametric way, robust against outliers, but with no confidence level:
lwr = yhat[101:length(yhat)] + boxplot.stats(noise, coef=1.5)$stats[1]
upr = yhat[101:length(yhat)] + boxplot.stats(noise, coef=1.5)$stats[5]
# This is a useful approach for anomaly detection
# points outside the intervals are outliers
# these are extreme outliers
lwr = yhat[101:length(yhat)] + boxplot.stats(noise, coef=3)$stats[1]
upr = yhat[101:length(yhat)] + boxplot.stats(noise, coef=3)$stats[5]

# Non-parametric way, even more robust against outliers
lwr = yhat[101:length(yhat)] - 2.4*mad(noise) 
upr = yhat[101:length(yhat)] + 2.4*mad(noise)
# for normal data, sd=1.48*mad

# For a symmetric distribution with zero mean, the mad is the 75th percentile of the distribution
# That means, the following interval has confidence level 50% (if noise satisfies those assumptions):
lwr = yhat[101:length(yhat)] - mad(noise) 
upr = yhat[101:length(yhat)] + mad(noise)
# it's called the probable error:  half of the values lies within the interval and half outside

predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)
predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))

# how many real observations are out of the intervals?
mean(predictions$out==1)
# the real coverage of parametric approach is around 15% and the nominal one is 10%, hence it's ok, 
# but the intervals should be a bit wider

# the real coverage of non-parametric approach is around 15% and the nominal one is 10%, hence it's ok, 
# but the intervals should be a bit wider

# the real coverage of robust approach is around 8%. Here, no nominal value

ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  xlim(20000, 1000000) + ylim(20000, 1000000)+
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "prediction",y="real price")


